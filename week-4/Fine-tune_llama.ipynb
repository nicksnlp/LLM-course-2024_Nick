{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwaDgeMvMIOj",
        "outputId": "b75a9df9-9eab-4029-df55-0c9069b98e80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.27.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.14.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.2.1)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.19.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec>=2023.5.0 (from huggingface_hub)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.5.1+cu121)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.25.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.10.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.19.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.12.14)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
            "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "# Install necessary dependencies\n",
        "!pip install huggingface_hub transformers datasets peft accelerate wandb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "VZszs0INMIOk"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import re\n",
        "from huggingface_hub import login\n",
        "from transformers import AutoModelForTokenClassification, AutoTokenizer, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "import torch\n",
        "import wandb\n",
        "from google.colab import drive\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "from google.colab import userdata\n",
        "HUGGING_API = userdata.get('HUGGINGFACE_READ_AND_WRITE')\n",
        "GOOGLE_API = userdata.get('GOOGLE_API_KEY')\n",
        "WANDB_key = userdata.get('WANDB')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "9xODWcdcMIOl"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Login to Hugging Face\n",
        "login(token=HUGGINGFACE_API)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "6dwpfw9vMIOl"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Login to Weights & Biases\n",
        "wandb.login(key=WANDB_key)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "MrLTbi8PMIOl"
      },
      "outputs": [],
      "source": [
        "\n",
        "run = wandb.init(project='llama-7b-hallucination', job_type=\"training\", anonymous=\"allow\", name=\"test_1\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "TYRv_2-JMIOl"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Mount Google Drive to save models\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "oVDga17eMIOm"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Step 5: Create and Prepare Dataset for Hallucination Detection\n",
        "# This dataset will have two fields: 'text' and 'labels'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "GTvUWMfhMIOm"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 1. Load your dataset\n",
        "data = [\n",
        "    {\"text\": \"The Eiffel Tower is located in Berlin, Germany.\", \"labels\": [0, 0, 0, 0, 0, 0, 1, 1, 1, 0]},  # Hallucinated words: \"Berlin\", \"Germany\"\n",
        "    {\"text\": \"The capital of France is Paris.\", \"labels\": [0, 0, 0, 0, 0, 0, 0]},  # Correct sentence\n",
        "    {\"text\": \"The Amazon River flows through Asia.\", \"labels\": [0, 0, 0, 0, 0, 1, 0]},  # Hallucinated word: \"Asia\"\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "eN1yTGNRMIOm"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 2. Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "J_Zt7-wRMIOm"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 3. Preprocessing function: Tokenizing by words and punctuation (treat punctuation as words)\n",
        "def preprocess_function(examples):\n",
        "    tokenized_input = []\n",
        "    all_labels = []\n",
        "\n",
        "    for idx, sentence in enumerate(examples['text']):\n",
        "        word_labels = examples['labels'][idx]  # Get the corresponding labels for the sentence\n",
        "\n",
        "        # Manually separate punctuation marks as individual \"words\"\n",
        "        sentence = re.sub(r\"([.,!?;])\", r\" \\1 \", sentence)  # Add spaces around punctuation marks\n",
        "\n",
        "        # Tokenize the sentence (this will now include punctuation as separate tokens)\n",
        "        word_tokens = tokenizer.tokenize(sentence)\n",
        "\n",
        "        # Split the sentence into words and punctuation for easier labeling\n",
        "        words = sentence.split()\n",
        "\n",
        "        # Ensure labels match the number of words\n",
        "        assert len(word_labels) == len(words), f\"Number of labels does not match the number of words in the sentence: {sentence}\"\n",
        "\n",
        "        # Create labels based on word-level tokens\n",
        "        expanded_labels = []\n",
        "        for word, label in zip(words, word_labels):\n",
        "            word_token_count = len(tokenizer.tokenize(word))\n",
        "            expanded_labels.extend([label] * word_token_count)  # Assign the label to each token of the word\n",
        "\n",
        "        # Tokenize the entire sentence to get input_ids and attention_mask\n",
        "        tokenized_sentence = tokenizer(sentence, padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "        # Append the tokenized input and its corresponding labels\n",
        "        tokenized_input.append(tokenized_sentence)\n",
        "        all_labels.append(expanded_labels)\n",
        "\n",
        "    return {\n",
        "        'input_ids': [item['input_ids'] for item in tokenized_input],\n",
        "        'attention_mask': [item['attention_mask'] for item in tokenized_input],\n",
        "        'labels': all_labels\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "3TW3lwh5MIOm"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 4. Convert data to Hugging Face dataset format\n",
        "dataset = Dataset.from_dict({\n",
        "    'text': [item['text'] for item in data],\n",
        "    'labels': [item['labels'] for item in data]\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "P1QX_uIoMIOn"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 5. Apply the preprocessing\n",
        "tokenized_data = dataset.map(preprocess_function, batched=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "SHDgXVR7MIOn"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 6. Check tokenized data\n",
        "print(\"Tokenized data:\")\n",
        "for item in tokenized_data:\n",
        "    print(item)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "JoFAbN1gMIOn"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load model for token classification\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"meta-llama/Llama-2-7b-hf\", num_labels=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "1gEG7omWMIOn"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Apply PEFT (LoRA)\n",
        "lora_config = LoraConfig(\n",
        "    r=8,  # Low-rank factor (adjust this based on your needs)\n",
        "    lora_alpha=16,  # Scaling factor for LoRA\n",
        "    lora_dropout=0.1,  # Dropout rate for LoRA\n",
        "    bias=\"none\"  # Specify whether or not to use biases in LoRA layers\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "izwBWswjMIOn"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Apply LoRA to the model\n",
        "model = get_peft_model(model, lora_config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "XTDk5fM4MIOn"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define label names (0 = correct, 1 = hallucinated)\n",
        "model.config.id2label = {0: \"correct\", 1: \"hallucinated\"}\n",
        "model.config.label2id = {\"correct\": 0, \"hallucinated\": 1}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "DZaMzhjJMIOn"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Set up training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/NLP/MODELS/FineTunedModel\",  # Path to save the model\n",
        "    evaluation_strategy=\"epoch\",  # Evaluate the model every epoch\n",
        "    learning_rate=2e-5,  # Learning rate\n",
        "    per_device_train_batch_size=4,  # Training batch size\n",
        "    per_device_eval_batch_size=8,  # Evaluation batch size\n",
        "    num_train_epochs=3,  # Number of epochs\n",
        "    weight_decay=0.01,  # Weight decay for optimization\n",
        "    logging_dir=\"/content/drive/MyDrive/NLP/MODELS/Logs\",  # Save logs to Google Drive\n",
        "    logging_steps=10,  # Log every 10 steps\n",
        "    push_to_hub=False,  # Set to True to upload the model after training\n",
        "    report_to=\"wandb\",  # Report metrics to Weights & Biases\n",
        "    run_name=\"test_1\",\n",
        "    resume_from_checkpoint=True,  # Automatically resume from the last checkpoint\n",
        "    save_steps=300,\n",
        "    save_total_limit=3, # keep only the last 3 checkpoints\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "qUeikuY9MIOn"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Initialize the Trainer with model, training arguments, and datasets\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_data,  # Use the tokenized dataset for training\n",
        "    eval_dataset=tokenized_data,  # Optional: Use the same dataset for evaluation\n",
        "    tokenizer=tokenizer,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "kmOdMfYBMIOn"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Start training\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "5C_0VrmxMIOn"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Save the fine-tuned model to Google Drive\n",
        "model_save_path = \"/content/drive/MyDrive/NLP/MODELS/FineTunedModel\"\n",
        "trainer.save_model(model_save_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "rz0pdlkoMIOn"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Optionally, push the fine-tuned model to Hugging Face Hub\n",
        "model.push_to_hub(\"your_huggingface_username/your_model_repo_name\")\n",
        "tokenizer.push_to_hub(\"your_huggingface_username/your_model_repo_name\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "LY00RPl1MIOn"
      },
      "outputs": [],
      "source": [
        "\n",
        "wandb.finish()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "F7C7Ud09MIOo"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Inference: Using the Fine-Tuned Model for Inference\n",
        "def infer_with_model(input_text):\n",
        "    # Tokenize the input text\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "\n",
        "    # Predict the token labels (hallucination vs. correct)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits  # Raw logits output from the model\n",
        "\n",
        "    # Get the predicted labels (0 for correct, 1 for hallucinated)\n",
        "    predicted_labels = torch.argmax(logits, dim=-1)\n",
        "\n",
        "    # Decode the tokens from the input text\n",
        "    tokens = tokenizer.tokenize(input_text)\n",
        "\n",
        "    # Get the corresponding predicted labels for each token\n",
        "    labeled_tokens = list(zip(tokens, predicted_labels[0].tolist()))\n",
        "\n",
        "    # Create a list of hallucinated words\n",
        "    hallucinated_words = [token for token, label in labeled_tokens if label == 1]\n",
        "\n",
        "    return hallucinated_words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "4Eki8QQiMIOo"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Example usage of the inference function\n",
        "input_text = \"The Eiffel Tower is located in Berlin, Germany.\"\n",
        "hallucinated_words = infer_with_model(input_text)\n",
        "\n",
        "# Print the list of hallucinated words\n",
        "print(\"Hallucinated words:\")\n",
        "print(hallucinated_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oM6ZyRiSO_u6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
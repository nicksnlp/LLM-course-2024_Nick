{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Install Dependencies\n",
    "!pip install huggingface_hub transformers datasets peft accelerate wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 2. Import necessary libraries\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "import wandb\n",
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 3. Use secrets for login (in Colab, these can be set via userdata)\n",
    "HUGGINGFACE_API_KEY = userdata.get('HUGGINGFACE_API_KEY')  # Hugging Face API Token\n",
    "WANDB_API_KEY = userdata.get('WANDB_API_KEY')  # Weights & Biases Key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Login to Hugging Face\n",
    "os.environ[\"HF_HOME\"] = \"/content/drive/MyDrive/NLP/MODELS/huggingface_cache\"  # Set Hugging Face cache directory if needed\n",
    "login(token=HUGGINGFACE_API_KEY)  # Login to Hugging Face using the API token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Login to Weights & Biases\n",
    "wandb.login(key=WANDB_API_KEY)  # Login to W&B using the API key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 4. Mount Google Drive to save models\n",
    "drive.mount('/content/drive/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 5. Load and Preprocess Dataset\n",
    "# Load the FEVER dataset (fact extraction and verification)\n",
    "dataset = load_dataset(\"fever\")\n",
    "\n",
    "# Inspect the first few examples from the dataset\n",
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 6. Tokenizer and Model Setup\n",
    "# Define the model we will fine-tune (Llama 2 - 7B)\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"  # You can change this to another model if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 7. Preprocessing Dataset\n",
    "# Select only a subset (5,000 samples) for training\n",
    "subset_dataset = dataset[\"train\"].select(range(5000))  # Select the first 5,000 samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Preprocessing function for tokenizing the dataset\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['sentence'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Apply preprocessing to the subset dataset\n",
    "tokenized_datasets = subset_dataset.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 8. Training Setup\n",
    "# Define the training arguments (hyperparameters)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/content/drive/MyDrive/NLP/MODELS/FineTunedModel\",  # Save model to specified Google Drive path\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate every epoch\n",
    "    learning_rate=2e-5,  # Learning rate\n",
    "    per_device_train_batch_size=4,  # Training batch size\n",
    "    per_device_eval_batch_size=8,  # Evaluation batch size\n",
    "    num_train_epochs=3,  # Number of epochs\n",
    "    weight_decay=0.01,  # Weight decay for optimization\n",
    "    logging_dir=\"/content/drive/MyDrive/NLP/MODELS/Logs\",  # Save logs to Google Drive\n",
    "    logging_steps=10,  # Log every 10 steps\n",
    "    push_to_hub=False,  # Set to True to upload the model after training\n",
    "    report_to=\"wandb\",  # Report metrics to Weights & Biases\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 9. Trainer Setup\n",
    "# Initialize the Trainer with the model, training arguments, and tokenized dataset\n",
    "trainer = Trainer(\n",
    "    model=model,  # The model to train\n",
    "    args=training_args,  # Training arguments\n",
    "    train_dataset=tokenized_datasets,  # Training dataset (5K subset)\n",
    "    eval_dataset=tokenized_datasets,  # Evaluation dataset (5K subset)\n",
    "    tokenizer=tokenizer,  # Tokenizer for preprocessing\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 10. Save and Upload the Model\n",
    "# Save the fine-tuned model to Google Drive\n",
    "model_save_path = \"/content/drive/MyDrive/NLP/MODELS/FineTunedModel\"\n",
    "trainer.save_model(model_save_path)  # Save the model locally in Drive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Optionally, push the fine-tuned model to Hugging Face Hub\n",
    "# Make sure to create a new model repo on Hugging Face first\n",
    "model.push_to_hub(\"your_huggingface_username/your_model_repo_name\")\n",
    "tokenizer.push_to_hub(\"your_huggingface_username/your_model_repo_name\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 11. Inference: Using the Fine-Tuned Model for Inference\n",
    "# Example inference with the fine-tuned model\n",
    "def infer_with_model(input_text):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    output = model.generate(**inputs, max_length=50)\n",
    "    decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return decoded_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Example usage of inference function\n",
    "input_text = \"What is the capital of France?\"\n",
    "response = infer_with_model(input_text)\n",
    "print(\"Model's response:\", response)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
